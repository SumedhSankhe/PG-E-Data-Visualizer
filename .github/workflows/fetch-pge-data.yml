name: Fetch PGE Data

on:
  schedule:
    # Run daily at 3 AM UTC (7 PM PST / 8 PM PDT)
    - cron: '0 3 * * *'
  workflow_dispatch:  # Allow manual triggering

# Prevent concurrent runs to avoid data conflicts
concurrency:
  group: pge-data-fetch
  cancel-in-progress: false

jobs:
  fetch-and-process:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 1  # Shallow clone for faster checkout

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install system dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y libcurl4-openssl-dev libssl-dev libxml2-dev

      - name: Set up R
        uses: r-lib/actions/setup-r@v2
        with:
          r-version: '4.4.0'
          use-public-rspm: true

      - name: Setup renv and cache R packages
        uses: r-lib/actions/setup-renv@v2
        with:
          cache-version: 2

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Install rsconnect (cached with renv)
        run: |
          if(!requireNamespace("rsconnect", quietly=TRUE)) install.packages("rsconnect")
        shell: Rscript {0}

      - name: Create data directory
        run: mkdir -p data

      - name: Fetch and parse PGE data from Supabase
        env:
          PGE_CLIENT_ID: ${{ secrets.PGE_CLIENT_ID }}
          PGE_CLIENT_SECRET: ${{ secrets.PGE_CLIENT_SECRET }}
          PGE_THIRD_PARTY_ID: ${{ secrets.PGE_THIRD_PARTY_ID }}
          PGE_CERT_CRT_BASE64: ${{ secrets.PGE_CERT_CRT_BASE64 }}
          PGE_CERT_KEY_BASE64: ${{ secrets.PGE_CERT_KEY_BASE64 }}
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
        run: |
          echo "Fetching data from Supabase and PGE API..."
          python scripts/automation/fetch_and_parse_pge.py

      - name: Process and merge data
        run: |
          echo "Processing data and merging with existing records..."
          Rscript scripts/automation/process_pge_data.R

      - name: Check for changes
        id: check_changes
        run: |
          # Check if database or RDS backup changed
          if git diff --quiet data/pge_meter_data.sqlite data/meterData.rds; then
            echo "changes=false" >> $GITHUB_OUTPUT
          else
            echo "changes=true" >> $GITHUB_OUTPUT
          fi

      - name: Commit and push changes
        if: steps.check_changes.outputs.changes == 'true'
        run: |
          git config user.name "GitHub Actions Bot"
          git config user.email "actions@github.com"
          git add data/pge_meter_data.sqlite
          git add data/meterData.rds
          git add logs/data-processing.log
          git commit -m "Auto-update: PGE data fetch $(date +'%Y-%m-%d %H:%M UTC')"
          git push
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

      - name: Deploy to shinyapps.io
        if: steps.check_changes.outputs.changes == 'true'
        env:
          SHINYAPPS_ACCOUNT: ${{ secrets.SHINYAPPS_ACCOUNT }}
          SHINYAPPS_TOKEN: ${{ secrets.SHINYAPPS_TOKEN }}
          SHINYAPPS_SECRET: ${{ secrets.SHINYAPPS_SECRET }}
        run: |
          Rscript -e "
          rsconnect::setAccountInfo(
            name = Sys.getenv('SHINYAPPS_ACCOUNT'),
            token = Sys.getenv('SHINYAPPS_TOKEN'),
            secret = Sys.getenv('SHINYAPPS_SECRET')
          )
          rsconnect::deployApp(
            appName = 'PG-E-Data-Visualizer',
            forceUpdate = TRUE,
            launch.browser = FALSE
          )
          "
        shell: bash

      - name: No changes detected
        if: steps.check_changes.outputs.changes == 'false'
        run: echo "No new data to commit"

      - name: Upload logs as artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: processing-logs-${{ github.run_number }}
          path: logs/
          retention-days: 30

      - name: Summary
        if: always()
        run: |
          echo "## PGE Data Fetch Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "- **Date:** $(date +'%Y-%m-%d %H:%M UTC')" >> $GITHUB_STEP_SUMMARY
          echo "- **Status:** ${{ job.status }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Changes:** ${{ steps.check_changes.outputs.changes }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          if [ -f logs/data-processing.log ]; then
            echo "### Processing Log (Last 20 lines)" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            tail -n 20 logs/data-processing.log >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
          fi
